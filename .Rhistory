new_stops <- c("coffee", "bean", stopwords("en"))
removeWords(text, new_stops)
complicate <- c("complicated", "complication", "complicatedly")
stem_doc <- stemDocument(complicate)
comp_dict <- ("complicate")
complete_text <- stemCompletion(stem_doc, comp_dict)
complete_text
stemDocument("In a complicated haste, Tom rushed to fix a new complication, too complicatedly.")
text <- "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."
library(qdap)
frequent_terms <- freq_terms(text, 3)
plot(frequent_terms)
library(readr)
# Import text data
tweets <- read_csv("data/NeildeGrasseTysonTweets.csv")
# View the structure of tweets
str(tweets)
# Print out the number of rows in tweets
nrow(tweets)
# Isolate text from tweets: tweets_text
tweets_text <- tweets$text
str(tweets_text)
library(tm)
tweets_source <- VectorSource(tweets_text)
# Make a volatile corpus: tweets_corpus
tweets_corpus <- VCorpus(tweets_source)
# Print out the tweets_corpus
tweets_corpus
# Print data on the 15th tweet in tweets_corpus
tweets_corpus[[15]]
# Print the content of the 15th tweet in tweets_corpus
tweets_corpus[[15]][1]
str(tweets_corpus[[15]])
example_text <- data.frame(num = c(1,2,3), Author1 = c("Text mining is a great time.", "Text analysis provides insights", "qdap and tm are used in text mining"), Author2 = c("R is a great language", "R has many uses", "R is cool!"), stringsAsFactors = FALSE)
# Create a DataframeSource on columns 2 and 3: df_source
df_source <- DataframeSource(example_text[, 2:3])
# Convert df_source to a corpus: df_corpus
df_corpus <- VCorpus(df_source)
# Examine df_corpus
df_corpus
str(df_corpus)
# Create a VectorSource on column 3: vec_source
vec_source <- VectorSource(example_text[, 3])
# Convert vec_source to a corpus: vec_corpus
vec_corpus <- VCorpus(vec_source)
# Examine vec_corpus
vec_corpus
str(vec_corpus)
# Create the object: text
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."
# All lowercase
tolower(text)
# Remove punctuation
removePunctuation(text)
# Remove numbers
removeNumbers(text)
# Remove whitespace
stripWhitespace(text)
# Remove text within brackets
bracketX(text)
# Replace numbers with words
replace_number(text)
# Replace abbreviations
replace_abbreviation(text)
# Replace contractions
replace_contraction(text)
# Replace symbols with words
replace_symbol(text)
# List standard English stop words
stopwords("en")
# Print text without standard stop words
removeWords(text, stopwords("en"))
# Add "coffee" and "bean" to the list: new_stops
new_stops <- c("coffee", "bean", stopwords("en"))
# Remove stop words from text
removeWords(text, new_stops)
# Create complicate
complicate <- c("complicated", "complication", "complicatedly")
# Perform word stemming: stem_doc
stem_doc <- stemDocument(complicate)
# Create the completion dictionary: comp_dict
comp_dict <- ("complicate")
# Perform stem completion: complete_text
complete_text <- stemCompletion(stem_doc, comp_dict)
# Print complete_text
complete_text
stemDocument("In a complicated haste, Tom rushed to fix a new complication, too complicatedly.")
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
text_data
rm_punc <- removePunctuation(text_data)
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
rm_punc <- removePunctuation(text_data)
n_char_vec <- unlist(strsplit(text_data, split = ' '))
n_char_vec
stem_doc <- stemDocument(n_char_vec)
stem_doc
stem_doc <- stemDocument(n_char_vec)
stem_doc
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)
# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(text_data, split = ' '))
# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec)
# Print stem_doc
stem_doc
# Re-complete stemmed document: complete_doc
complete_doc <- stemCompletion(stem_doc, comp_dict)
# Print complete_doc
complete_doc
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
rm_punc <- removePunctuation(text_data)
n_char_vec <- unlist(strsplit(text_data, split = ' '))
stem_doc <- stemDocument(n_char_vec)
stem_doc
complete_doc <- stemCompletion(stem_doc, comp_dict)
complete_doc
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
rm_punc <- removePunctuation(text_data)
n_char_vec <- unlist(strsplit(text_data, split = ' '))
n_char_vec
str(text_data)
rm_punc <- removePunctuation(text_data)
rm_punc
n_char_vec <- unlist(strsplit(rm_punc, split = ' '))
stem_doc <- stemDocument(n_char_vec)
stem_doc
comp_dict <- ("complicate")
complete_doc <- stemCompletion(stem_doc, comp_dict)
complete_doc
comp_dict <- c("In", "a", "complicate", "haste", "Tom", "rush", "to", "fix", "new", "too")
complete_doc <- stemCompletion(stem_doc, comp_dict)
complete_doc
text <- "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."
library(qdap)
frequent_terms <- freq_terms(text, 3)
plot(frequent_terms)
library(readr)
# Import text data
tweets <- read_csv("data/NeildeGrasseTysonTweets.csv")
# View the structure of tweets
str(tweets)
# Print out the number of rows in tweets
nrow(tweets)
# Isolate text from tweets: tweets_text
tweets_text <- tweets$text
str(tweets_text)
library(tm)
tweets_source <- VectorSource(tweets_text)
# Make a volatile corpus: tweets_corpus
tweets_corpus <- VCorpus(tweets_source)
# Print out the tweets_corpus
tweets_corpus
# Print data on the 15th tweet in tweets_corpus
tweets_corpus[[15]]
# Print the content of the 15th tweet in tweets_corpus
tweets_corpus[[15]][1]
str(tweets_corpus[[15]])
example_text <- data.frame(num = c(1,2,3), Author1 = c("Text mining is a great time.", "Text analysis provides insights", "qdap and tm are used in text mining"), Author2 = c("R is a great language", "R has many uses", "R is cool!"), stringsAsFactors = FALSE)
# Create a DataframeSource on columns 2 and 3: df_source
df_source <- DataframeSource(example_text[, 2:3])
# Convert df_source to a corpus: df_corpus
df_corpus <- VCorpus(df_source)
# Examine df_corpus
df_corpus
str(df_corpus)
# Create a VectorSource on column 3: vec_source
vec_source <- VectorSource(example_text[, 3])
# Convert vec_source to a corpus: vec_corpus
vec_corpus <- VCorpus(vec_source)
# Examine vec_corpus
vec_corpus
str(vec_corpus)
# Create the object: text
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."
# All lowercase
tolower(text)
# Remove punctuation
removePunctuation(text)
# Remove numbers
removeNumbers(text)
# Remove whitespace
stripWhitespace(text)
# Remove text within brackets
bracketX(text)
# Replace numbers with words
replace_number(text)
# Replace abbreviations
replace_abbreviation(text)
# Replace contractions
replace_contraction(text)
# Replace symbols with words
replace_symbol(text)
# List standard English stop words
stopwords("en")
# Print text without standard stop words
removeWords(text, stopwords("en"))
# Add "coffee" and "bean" to the list: new_stops
new_stops <- c("coffee", "bean", stopwords("en"))
# Remove stop words from text
removeWords(text, new_stops)
# Create complicate
complicate <- c("complicated", "complication", "complicatedly")
# Perform word stemming: stem_doc
stem_doc <- stemDocument(complicate)
# Create the completion dictionary: comp_dict
comp_dict <- ("complicate")
# Perform stem completion: complete_text
complete_text <- stemCompletion(stem_doc, comp_dict)
# Print complete_text
complete_text
stemDocument("In a complicated haste, Tom rushed to fix a new complication, too complicatedly.")
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)
# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = ' '))
# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec)
# Print stem_doc
stem_doc
# Create the completion dictionary: comp_dict
comp_dict <- c("In", "a", "complicate", "haste", "Tom", "rush", "to", "fix", "new", "too")
# Re-complete stemmed document: complete_doc
complete_doc <- stemCompletion(stem_doc, comp_dict)
# Print complete_doc
complete_doc
# Create the function that will be used to clean the corpus
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
return(corpus)
}
# Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(tweet_corp)
frequent_terms <- freq_terms(tweets_text, 10)
plot(freq_terms())
plot(frequent_terms)
frequent_terms <- freq_terms(tweets_text, 50)
plot(frequent_terms)
frequent_terms <- freq_terms(tweets_text, 30)
plot(frequent_terms)
#Let's find the most frequent words in our tweets_text and see whether we should get rid of some
frequent_terms <- freq_terms(tweets_text, 30)
plot(frequent_terms)
# Well nothing stands out in particular, exepct ties and articles, so the standard wocabulary of stopwords
# in English will do just fine.
# Create the custom function that will be used to clean the corpus: clean_coupus
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("en"))
return(corpus)
}
# Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(tweets_corpus)
# Print out a cleaned up tweet
clean_corp[[227]][1]
# Print out the same tweet in original form
tweets$text[227]
frequent_terms <- freq_terms(clean_corp, 30)
plot(frequent_terms)
#Let's find the most frequent words in our tweets_text and see whether we should get rid of some
frequent_terms <- freq_terms(tweets_text, 30)
plot(frequent_terms)
# Well nothing stands out in particular, exepct ties and articles, so the standard wocabulary of stopwords
# in English will do just fine.
# Create the custom function that will be used to clean the corpus: clean_coupus
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("en"))
return(corpus)
}
# Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(tweets_corpus)
# Let's check the word counts again
frequent_terms <- freq_terms(clean_corp, 30)
plot(frequent_terms)
# Print out a cleaned up tweet
clean_corp[[227]][1]
# Print out the same tweet in original form
tweets$text[227]
frequent_terms <- freq_terms(tweets_text, 30)
plot(frequent_terms)
frequent_terms <- freq_terms(clean_corp, 30)
frequent_terms <- freq_terms(clean_corp, 30)
plot(frequent_terms)
#Let's find the most frequent words in our tweets_text and see whether we should get rid of some
frequent_terms <- freq_terms(tweets_text, 30)
plot(frequent_terms)
# Well nothing stands out in particular, exepct ties and articles, so the standard wocabulary of stopwords
# in English will do just fine.
# Create the custom function that will be used to clean the corpus: clean_coupus
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("en"))
return(corpus)
}
# Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(tweets_corpus)
# Print out a cleaned up tweet
clean_corp[[227]][1]
# Print out the same tweet in original form
tweets$text[227]
# Create the dtm from the corpus:
tweets_dtm <- DocumentTermMatrix(clean_corp)
# Print out tweets_dtm data
tweets_dtm
# Convert tweets_dtm to a matrix: tweets_m
tweets_m <- as.matrix(tweets_dtm)
# Print the dimensions of tweets_m
dim(tweets_m)
# Review a portion of the matrix
tweets_m[148:150, 2587:2590]
tweets_dtm_sparse_rem <- removeSparseTerms(tweets_dtm, 0.95)
rm(tweets_dtm_sparse_rem)
tweets_dtm_rm_sparse <- removeSparseTerms(tweets_dtm, 0.95)
tweets_dtm_rm_sparse
tweets_m <- as.matrix(tweets_dtm_rm_sparse)
dim(tweets_m)
tweets_m[148:150, 2587:2590]
tweets_m[148:150, 2:4]
tweets_dtm_rm_sparse <- removeSparseTerms(tweets_dtm, 0.98)
tweets_dtm_rm_sparse
tweets_m <- as.matrix(tweets_dtm_rm_sparse)
dim(tweets_m)
tweets_m[148:150, 2:4]
tweets_m[148:158, 10:35]
tweets_m[148:158, 10:22]
# Create the tdm from the corpus:
tweets_tdm <- TermDocumentMatrix(clean_corp)
# Print out tweets_tdm data
tweets_tdm
# Convert tweets_tdm to a matrix: tweets_m
tweets_m <- as.matrix(tweets_tdm)
# Print the dimensions of tweets_m
dim(tweets_m)
# Review a portion of the matrix
tweets_m[148:150, 2587:2590]
tweets_m[148:158, 126:138]
tweets_tdm_rm_sparse <- removeSparseTerms(tweets_tdm, 0.98)
tweets_tdm_rm_sparse
tweets_m <- as.matrix(tweets_tdm_rm_sparse)
dim(tweets_m)
tweets_tdm_rm_sparse <- removeSparseTerms(tweets_tdm, 0.99)
tweets_tdm_rm_sparse
tweets_m <- as.matrix(tweets_tdm_rm_sparse)
dim(tweets_m)
tweets_m[14:28, 10:22]
text <- "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."
library(qdap)
frequent_terms <- freq_terms(text, 3)
plot(frequent_terms)
library(readr)
# Import text data
tweets <- read_csv("data/NeildeGrasseTysonTweets.csv")
# View the structure of tweets
str(tweets)
# Print out the number of rows in tweets
nrow(tweets)
# Isolate text from tweets: tweets_text
tweets_text <- tweets$text
str(tweets_text)
library(tm)
tweets_source <- VectorSource(tweets_text)
# Make a volatile corpus: tweets_corpus
tweets_corpus <- VCorpus(tweets_source)
# Print out the tweets_corpus
tweets_corpus
# Print data on the 15th tweet in tweets_corpus
tweets_corpus[[15]]
# Print the content of the 15th tweet in tweets_corpus
tweets_corpus[[15]][1]
str(tweets_corpus[[15]])
example_text <- data.frame(num = c(1,2,3), Author1 = c("Text mining is a great time.", "Text analysis provides insights", "qdap and tm are used in text mining"), Author2 = c("R is a great language", "R has many uses", "R is cool!"), stringsAsFactors = FALSE)
# Create a DataframeSource on columns 2 and 3: df_source
df_source <- DataframeSource(example_text[, 2:3])
# Convert df_source to a corpus: df_corpus
df_corpus <- VCorpus(df_source)
# Examine df_corpus
df_corpus
str(df_corpus)
# Create a VectorSource on column 3: vec_source
vec_source <- VectorSource(example_text[, 3])
# Convert vec_source to a corpus: vec_corpus
vec_corpus <- VCorpus(vec_source)
# Examine vec_corpus
vec_corpus
str(vec_corpus)
# Create the object: text
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."
# All lowercase
tolower(text)
# Remove punctuation
removePunctuation(text)
# Remove numbers
removeNumbers(text)
# Remove whitespace
stripWhitespace(text)
# Remove text within brackets
bracketX(text)
# Replace numbers with words
replace_number(text)
# Replace abbreviations
replace_abbreviation(text)
# Replace contractions
replace_contraction(text)
# Replace symbols with words
replace_symbol(text)
# List standard English stop words
stopwords("en")
# Print text without standard stop words
removeWords(text, stopwords("en"))
# Add "coffee" and "bean" to the list: new_stops
new_stops <- c("coffee", "bean", stopwords("en"))
# Remove stop words from text
removeWords(text, new_stops)
# Create complicate
complicate <- c("complicated", "complication", "complicatedly")
# Perform word stemming: stem_doc
stem_doc <- stemDocument(complicate)
# Create the completion dictionary: comp_dict
comp_dict <- ("complicate")
# Perform stem completion: complete_text
complete_text <- stemCompletion(stem_doc, comp_dict)
# Print complete_text
complete_text
stemDocument("In a complicated haste, Tom rushed to fix a new complication, too complicatedly.")
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)
# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = ' '))
# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec)
# Print stem_doc
stem_doc
# Create the completion dictionary: comp_dict
comp_dict <- c("In", "a", "complicate", "haste", "Tom", "rush", "to", "fix", "new", "too")
# Re-complete stemmed document: complete_doc
complete_doc <- stemCompletion(stem_doc, comp_dict)
# Print complete_doc
complete_doc
#Let's find the most frequent words in our tweets_text and see whether we should get rid of some
frequent_terms <- freq_terms(tweets_text, 30)
plot(frequent_terms)
# Well nothing stands out in particular, exepct ties and articles, so the standard wocabulary of stopwords
# in English will do just fine.
# Create the custom function that will be used to clean the corpus: clean_coupus
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("en"))
return(corpus)
}
# Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(tweets_corpus)
# Print out a cleaned up tweet
clean_corp[[227]][1]
# Print out the same tweet in original form
tweets$text[227]
# Create the dtm from the corpus:
tweets_dtm <- DocumentTermMatrix(clean_corp)
# Print out tweets_dtm data
tweets_dtm
# Convert tweets_dtm to a matrix: tweets_m
tweets_m <- as.matrix(tweets_dtm)
# Print the dimensions of tweets_m
dim(tweets_m)
# Review a portion of the matrix
tweets_m[148:150, 2587:2590]
# Since the sparsity is so high, i.e. a proportion of cells with 0s/ cells with other values is too large,
# let's remove some of these low frequency terms
tweets_dtm_rm_sparse <- removeSparseTerms(tweets_dtm, 0.98)
# Print out tweets_dtm data
tweets_dtm_rm_sparse
# Convert tweets_dtm to a matrix: tweets_m
tweets_m <- as.matrix(tweets_dtm_rm_sparse)
# Print the dimensions of tweets_m
dim(tweets_m)
# Review a portion of the matrix
tweets_m[148:158, 10:22]
# Create the tdm from the corpus:
tweets_tdm <- TermDocumentMatrix(clean_corp)
# Print out tweets_tdm data
tweets_tdm
# Convert tweets_tdm to a matrix: tweets_m
tweets_m <- as.matrix(tweets_tdm)
# Print the dimensions of tweets_m
dim(tweets_m)
# Review a portion of the matrix
tweets_m[148:158, 126:138]
# Since the sparsity is so high, i.e. a proportion of cells with 0s/ cells with other values is too large,
# let's remove some of these low frequency terms
tweets_tdm_rm_sparse <- removeSparseTerms(tweets_tdm, 0.99)
# Print out tweets_dtm data
tweets_tdm_rm_sparse
# Convert tweets_dtm to a matrix: tweets_m
tweets_m <- as.matrix(tweets_tdm_rm_sparse)
# Print the dimensions of tweets_m
dim(tweets_m)
# Review a portion of the matrix
tweets_m[14:28, 10:22]
